퍼셉트론과 신경망의 결정적인 차이는 활성화함수에 있다고 들었다. 퍼셉트론의 활성화함수는 계단함수처럼 생겼다.  
임계값이 몇이냐에 따라 계단함수 수식에 +a가 될 것이다. 신경망의 활성화함수는 퍼셉트론의 결과값 0,1 처럼 나오지 않는다.  
0~1 사이에 소수로 나올 것이다. 이런 활성화함수를 가진 신경망이 퍼셉트론보다 강력하고 복잡한 문제를 풀 수 있는 이유가 뭘까 생각해봤다.  
신경망을 생각해보자. n개의 데이터가 들어오면 그 데이터들은 중요도나 영향력이 다 다를 것이다. 그런데 퍼셉트론의 활성함수를 사용한다고 생각해보자.  
중요도가 조금이라도 존재하면 전부다 1을 출력할 테지만, 신경망의 활성화함수를 사용하면 각 중요도에 따라서 결과값이 크고 작게 나올 것이다.  
이 점이 cnn이나 transformer 등에서 이미지의 특징이나 자연어의 특징, 시계열 데이터들을 분석하거나 찾아낼 수 있게 되는거라고 본다.  

생각  
활성화함수로 ReLU를 주로 사용한다고 알고있다. 이 이유는 특징들을 그대로 살릴 수 있기 때문이라고 본다. [sum(인풋값+가중치)+편향] 이 값을 있는 그대로 출력하는 함수가 ReLU이다.  
대신 그 값이 0이거나 0보다 작으면 정보로써 가치가 없거나 오히려 방해되는 값일 것이기 때문에 0으로 출력되어야 한다고 본다.  

질문  
1. 퍼셉트론이 아닌 신경망 출력층에 활성화 함수를 계단 함수로 쓴다면 복잡한 문제를 풀 수 있을까? 풀 수 있다면 왜일까. 풀 수 없다면 왜일까.  

2. 역전파는 퍼셉트론에서 사용할 수 있을까? 사용한다면 어떤 방법으로? 결과는 어떻게 될까? 안된다면 그 이유는?  

3. 신경망의 출력층에서 사용하는 활성화 함수는 왜 문제의 종류에 따라서 다르게 써야 하는가? 예를 들어 회귀문제에서는 선형함수, 이진 분류에서는 sigmoid, 다중 분류에서는 softmax처럼.  

4. 출력층에서 사용하는 손실함수는 어떤 기준으로 선택할까?  
   4-1. 0~1로 출력하는 게 언제 유리하고 언제 불리할까?
   4-2. 정답이 ‘확률’처럼 해석되는 상황과, 실제 ‘값’으로 해석되는 상황은 어떻게 구별할 수 있을까?  

5. 은닉층에서는 ReLU, tanh, sigmoid 같은 다양한 활성화 함수를 쓰는데 왜 softmax는 잘 안 쓰는가?

6. 생각 부분에서 다음 글을 썼는데 이 글은 과연 오류가 없는가? 문제가 생기는 상황은 없는가?
   활성화함수로 ReLU를 주로 사용한다고 알고있다. 이 이유는 특징들을 그대로 살릴 수 있기 때문이라고 본다. [sum(인풋값+가중치)+편향] 이 값을 있는 그대로 출력하는 함수가 ReLU이다.  
   대신 그 값이 0이거나 0보다 작으면 정보로써 가치가 없거나 오히려 방해되는 값일 것이기 때문에 0으로 출력되어야 한다고 본다.  
