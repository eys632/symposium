퍼셉트론과 신경망의 결정적인 차이는 활성화함수에 있다고 들었다. 퍼셉트론의 활성화함수는 계단함수처럼 생겼다.  
임계값이 몇이냐에 따라 계단함수 수식에 +a가 될 것이다. 신경망의 활성화함수는 퍼셉트론의 결과값 0,1 처럼 나오지 않는다.  
0~1 사이에 소수로 나올 것이다. 이런 활성화함수를 가진 신경망이 퍼셉트론보다 강력하고 복잡한 문제를 풀 수 있는 이유가 뭘까 생각해봤다.  
신경망을 생각해보자. n개의 데이터가 들어오면 그 데이터들은 중요도나 영향력이 다 다를 것이다. 그런데 퍼셉트론의 활성함수를 사용한다고 생각해보자.  
중요도가 조금이라도 존재하면 전부다 1을 출력할 테지만, 신경망의 활성화함수를 사용하면 각 중요도에 따라서 결과값이 크고 작게 나올 것이다.  
이 점이 cnn이나 transformer 등에서 이미지의 특징이나 자연어의 특징, 시계열 데이터들을 분석하거나 찾아낼 수 있게 되는거라고 본다.  

생각  
활성화함수로 ReLU를 주로 사용한다고 알고있다. 이 이유는 특징들을 그대로 살릴 수 있기 때문이라고 본다. [sum(인풋값+가중치)+편향] 이 값을 있는 그대로 출력하는 함수가 ReLU이다.  
대신 그 값이 0이거나 0보다 작으면 정보로써 가치가 없거나 오히려 방해되는 값일 것이기 때문에 0으로 출력되어야 한다고 본다.  

질문  
1. 퍼셉트론이 아닌 신경망 출력층에 활성화 함수를 계단 함수로 쓴다면 복잡한 문제를 풀 수 있을까? 풀 수 있다면 왜일까. 풀 수 없다면 왜일까.  

2. 역전파는 퍼셉트론에서 사용할 수 있을까? 사용한다면 어떤 방법으로? 결과는 어떻게 될까? 안된다면 그 이유는?  

3. 신경망의 출력층에서 사용하는 활성화 함수와 은닉층에서 사용하는 활성화 함수는 달라도 되는가? 된다면 은닉층에서 사용하는 활성화 함수들도 각자 다 달라도 되는것인가? 그렇게 하면 성능 향상을 기대할 수 있나?  

4. 출력층에서 사용하는 손실함수는 어떤 기준으로 선택할까?  
   4-1. 0~1로 출력하는 게 언제 유리하고 언제 불리할까?  
   4-2. 정답이 ‘확률’처럼 해석되는 상황과, 실제 ‘값’으로 해석되는 상황은 어떻게 구별할 수 있을까?  

5. 은닉층에서는 ReLU, tanh, sigmoid 같은 다양한 활성화 함수를 쓰는데 왜 softmax는 잘 안 쓰는가? 잘 안쓰나? 만약 안쓴다면 그 이유가 뭔가?   

6. 생각 부분에서 다음 글을 썼는데 이 글은 과연 오류가 없는가? 문제가 생기는 상황은 없는가?  
   "활성화함수로 ReLU를 주로 사용한다고 알고있다. 이 이유는 특징들을 그대로 살릴 수 있기 때문이라고 본다. [sum(인풋값+가중치)+편향] 이 값을 있는 그대로 출력하는 함수가 ReLU이다.  
   대신 그 값이 0이거나 0보다 작으면 정보로써 가치가 없거나 오히려 방해되는 값일 것이기 때문에 0으로 출력되어야 한다고 본다."  

답변  
1. 복잡한 문제는 풀 수 없다. 예를 들어서 생각해보자. n개의 데이터가 들어오면 그 데이터들은 중요도나 영향력이 다 다를것이다. 그런데 퍼셉트론의 계단함수를 사용해버리면 중요도가 조금이라도 존재하면 전부다 1을 출력할 테지만,  
   신경망의 활성화함수를 사용하면 각 중요도에 따라서 결과값이 크고 작게 나올 것이다. 여기서 말하는 중요도는 노드의 값과 가중치를 곱한 값을 전부 더한 값을 말한다. 신이 알고있는 주어진 데이터에 가장 최적의 가중치와 편향값을 우리도 알고 있다고 해보자.  
   비선형 활성화 함수를 사용하면 진짜 중요한 것들을 강조할 수 있고 별로 중요하지 않은 것은 0을 출력하게 하거나 0에 가깝게 나오도록 조정할 수 있다.  

2. 퍼셉트론의 활성화 함수가 계단함수를 사용한다면 역전파는 의미가 없어진다. 순전파를 통해 찾은 함수에 test데이터를 대입하여 실제 값과 f(x)값을 비교한 다음(Loss) 해당 값에서 미분한 기울기를 확인하고 그것을 기준으로 Loss를 줄여나가야 하는데  
   계단함수는 어느 값에서 미분해도 0이 나오기 때문에 의미가 없어질 것이다.

3. 직접 실험을 해볼 필요가 있다고 본다. 상황이나 데이터에 따라서 더 좋아질 수 있기 때문이다.

4. 각각의 데이터들이 가지는 의미가 항상 달라지고 문제를 해결해야 하는 상황도 항상 바뀌기 때문에 활성화함수도 여러가지가 있는것이다. 여러가지의 feature가 있다고 할때 어떤 feature의 값은 maximaize해야 하고 어떤 feature값은 minimize해야 할 수도 있다.  
   이런 상황이라면 x값이 음수여도 반영 가능한 sigmoid함수를 쓰는 방법을 선택할 수 있다. 그리고 모든 feature들이 동일하게 최대화, 최소화해야 하는 상황에서는 ReLU를 사용하는 방법을 선택할 수도 있다.  
   즉, 결론적으로 feature들과 문제상황, 목적에 따라서 최적의 하이퍼파라미터가 달라지는 것이다.

5. 기울기 소실 때문이 아닐까 하는 생각을 한다. softmax는 출력값들의 모든 값들을 다 더하면 1이 되는 함수라고 알고있다. 내가 알고있는게 맞다고 가정하고 예시를 들어보자. mnist문제에서 7과 9는 그 형태가 비슷하다.  
   그럼 은닉층에서 7과 9로 예측하는 비율이 많은 레이어를 거칠수록 커지다가 어느새 7이냐 9냐 판단하는 레이어까지 도달하게 될 것이다. 여기서 특징들을 잘 학습했다면 둘중에 하나를 성공적으로 분류될 것이다.  
   이 과정에 도달하기까지 활성화함수가 softmax라면 너무 오래걸리기도 하고 도중에 죽은 노드가 발생할 수도 있다고 본다. 역전파를 할때도 위험할 것 같다.  

6. 은닉층에서 ReLU를 쓰는 이유는 아까 말했던 것처럼 특징들을 그대로 살릴 수 있기 때문이라고 봐. sum(인풋값+가중치)+편향 이 값을 있는 그대로 출력하는 함수가 ReLU이기 때문이지. 대신 그 값이 0이거나 0보다 작으면 정보로써 가치가 없거나 오히려 방해된다는 뜻이니까.  
   아까 말했던 방해가 되는 정보들을 참고해서 더 fit하게 학습하려고 leaky ReLU함수를 사용하는 것 같아. 내가 아까 말했던 내용중에 놓친 내용이 있는 것 같아. "값이 0이거나 0보다 작으면 정보로써 가치가 없거나 오히려 방해된다는 뜻이니까.".  
   하이퍼파라미터가 잘 학습되었다는 가정을 세워놓고 생각해보자. sum(인풋값+가중치)+편향 값이 마이너스라면 그것은 모델이 학습하게 되면 성능에 악영향을 주는 정보일 것이라고 생각돼. 그래서 leaky ReLU를 사용하는 것 같아.  
   leaky ReLU는 0보다 작으면 음수를 출력하는 정도(0~-무한대 범위의 기울기)가 굉장히 작잖아. 그래서 그런 것 같아.  
