# 0. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import os
import tensorflow as tf
import numpy as np

# TensorFlowê°€ ì´ˆê¸°í™”ë˜ê¸° ì „ì— ì‹¤í–‰í•˜ì—¬ 2ë²ˆ GPUë§Œ ì‚¬ìš©í•˜ë„ë¡ ê°•ì œ
os.environ["CUDA_VISIBLE_DEVICES"] = "2"
print(f"TensorFlowê°€ ì¸ì‹í•˜ëŠ” GPU: {tf.config.list_physical_devices('GPU')}")


# 1. ë°ì´í„° ì¤€ë¹„ ë° ì „ì—­ ë³€ìˆ˜ ì„¤ì •

# ë¡œê·¸ íŒŒì¼ì„ ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
LOG_DIR = "/home/a202192020/ë‚´_ê³µë¶€/ì‹ ê²½ë§ë¸”ë™ë°•ìŠ¤/pyíŒŒì¼"
os.makedirs(LOG_DIR, exist_ok=True)

# MNIST ë°ì´í„° ë¡œë“œ ë° ì •ê·œí™”
mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

# ë ˆì´ë¸”ì„ ì´ì§„ í˜•íƒœë¡œ ë³€í™˜ (3ì´ë©´ 1, ì•„ë‹ˆë©´ 0)
y_train_binary = (y_train == 3).astype(np.float32)
y_test_binary = (y_test == 3).astype(np.float32)


# ë°©ëŒ€í•œ ë¡œê·¸ ìƒì„±ì„ í”¼í•˜ê¸° ìœ„í•´ ë°ì´í„°ì˜ ì¼ë¶€(128ê°œ)ë§Œ ì‚¬ìš©í•˜ì—¬ ì‹¤í—˜ ì§„í–‰
# ë°°ì¹˜ í¬ê¸°ë¥¼ 64ë¡œ ì„¤ì •í•˜ë©´ ì´ 2ê°œì˜ ìŠ¤í…(Step)ì´ ì‹¤í–‰ë¨
BATCH_SIZE = 64
DATASET_SIZE = 128
train_dataset = tf.data.Dataset.from_tensor_slices((x_train[:DATASET_SIZE], y_train_binary[:DATASET_SIZE])).batch(BATCH_SIZE)


# 2. ì‹¤í—˜ ì‹¤í–‰ í•¨ìˆ˜ ì •ì˜

def run_experiment(model_type, log_file_name):
    """
    ì£¼ì–´ì§„ ëª¨ë¸ íƒ€ì…(ëŒ€ì¹­/í‘œì¤€)ì— ë”°ë¼ ì‹¤í—˜ì„ ì‹¤í–‰í•˜ê³  ìƒì„¸ ë¡œê·¸ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤.
    """
    # 2-1. ëª¨ë¸ ìƒì„±
    if model_type == 'symmetric':
        # ê°€ì„¤ ëª¨ë¸(A): ëª¨ë“  ê°€ì¤‘ì¹˜ë¥¼ 1, í¸í–¥ì„ 0ìœ¼ë¡œ ì´ˆê¸°í™”
        initializer_weights = tf.keras.initializers.Ones()
        initializer_bias = tf.keras.initializers.Zeros()
        model_name = "ëª¨ë¸ A (ê°€ì„¤-ëŒ€ì¹­ ëª¨ë¸)"
    else:
        # í‘œì¤€ ëª¨ë¸(B): Keras ê¸°ë³¸ ì´ˆê¸°í™” ì‚¬ìš© (Glorot Uniform)
        initializer_weights = tf.keras.initializers.GlorotUniform()
        initializer_bias = tf.keras.initializers.Zeros()
        model_name = "ëª¨ë¸ B (í‘œì¤€-ë¬´ì‘ìœ„ ëª¨ë¸)"

    # ëª¨ë¸ì˜ ê° ì¸µ ì •ì˜
    flatten_layer = tf.keras.layers.Flatten(input_shape=(28, 28))
    hidden_layer = tf.keras.layers.Dense(128, activation='relu', kernel_initializer=initializer_weights, bias_initializer=initializer_bias)
    output_layer = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=initializer_weights, bias_initializer=initializer_bias)
    all_layers = [hidden_layer, output_layer]

    # 2-2. í•™ìŠµ ì„¤ì •
    optimizer = tf.keras.optimizers.Adam()
    loss_fn = tf.keras.losses.BinaryCrossentropy()
    accuracy_metric = tf.keras.metrics.BinaryAccuracy()

    # 2-3. ë¡œê·¸ íŒŒì¼ ì„¤ì • ë° í•™ìŠµ ì‹œì‘
    log_file_path = os.path.join(LOG_DIR, log_file_name)
    
    with open(log_file_path, "w", encoding='utf-8') as f:
        f.write("="*60 + "\n")
        f.write(f"ğŸ”¬ [{model_name}] ë¸”ë™ë°•ìŠ¤ ë¡œê·¸ ì‹œì‘\n")
        f.write("="*60 + "\n")

        # 1 ì—í¬í¬ë§Œ ì‹¤í–‰
        for epoch in range(1):
            f.write(f"\n--- EPOCH {epoch+1} ---\n")
            accuracy_metric.reset_states() # ì—í¬í¬ ì‹œì‘ ì‹œ ì •í™•ë„ ì´ˆê¸°í™”

            for step, (x_batch, y_batch) in enumerate(train_dataset):
                f.write(f"\n{'='*25} STEP {step+1} {'='*25}\n")

                with tf.GradientTape() as tape:
                    # 3-1. ìˆœì „íŒŒ (Forward Propagation) ê¸°ë¡
                    f.write("\n--[ 3-1. ìˆœì „íŒŒ (Forward Propagation) ]--\n")
                    
                    x_flattened = flatten_layer(x_batch)
                    hidden_activations = hidden_layer(x_flattened)
                    
                    f.write("\n[ì€ë‹‰ì¸µ ì •ë³´]\n")
                    f.write(f"  - ê°€ì¤‘ì¹˜ (Shape: {hidden_layer.kernel.shape}):\n{np.array2string(hidden_layer.kernel.numpy(), threshold=np.inf, max_line_width=np.inf)}\n")
                    f.write(f"  - í¸í–¥ (Shape: {hidden_layer.bias.shape}):\n{np.array2string(hidden_layer.bias.numpy(), threshold=np.inf, max_line_width=np.inf)}\n")
                    f.write(f"  - í™œì„±í™” ê°’ (Shape: {hidden_activations.shape}):\n{np.array2string(hidden_activations.numpy(), threshold=np.inf, max_line_width=np.inf)}\n")

                    logits = output_layer(hidden_activations)
                    
                    f.write("\n[ì¶œë ¥ì¸µ ì •ë³´]\n")
                    f.write(f"  - ê°€ì¤‘ì¹˜ (Shape: {output_layer.kernel.shape}):\n{np.array2string(output_layer.kernel.numpy(), threshold=np.inf, max_line_width=np.inf)}\n")
                    f.write(f"  - í¸í–¥ (Shape: {output_layer.bias.shape}):\n{np.array2string(output_layer.bias.numpy(), threshold=np.inf, max_line_width=np.inf)}\n")
                    f.write(f"  - ìµœì¢… ì˜ˆì¸¡ ê°’ (Logits):\n{np.array2string(logits.numpy(), threshold=np.inf, max_line_width=np.inf)}\n")
                    
                    y_batch_expanded = tf.expand_dims(y_batch, axis=1)
                    loss_value = loss_fn(y_batch_expanded, logits)
                    
                    # 3-2. ì„±ëŠ¥ ì§€í‘œ ê¸°ë¡
                    accuracy_metric.update_state(y_batch_expanded, logits)
                    f.write("\n--[ 3-2. ì„±ëŠ¥ ì§€í‘œ ]--\n")
                    f.write(f"  - ì†ì‹¤ (Loss): {loss_value.numpy()}\n")
                    f.write(f"  - í˜„ì¬ ìŠ¤í…ê¹Œì§€ì˜ ëˆ„ì  ì •í™•ë„ (Cumulative Accuracy): {accuracy_metric.result().numpy()}\n")

                    # 3-3. ì—­ì „íŒŒ (Backpropagation) ê¸°ë¡
                    f.write("\n--[ 3-3. ì—­ì „íŒŒ (Backpropagation) ]--\n")
                    trainable_vars = [var for layer in all_layers for var in layer.trainable_variables]
                    grads = tape.gradient(loss_value, trainable_vars)
                
                    for i, layer in enumerate(all_layers):
                        layer_name = 'ì€ë‹‰ì¸µ' if i == 0 else 'ì¶œë ¥ì¸µ'
                        f.write(f"\n[{layer_name}ì˜ ê¸°ìš¸ê¸°(Gradients)]\n")
                        f.write(f"  - ê°€ì¤‘ì¹˜ ê¸°ìš¸ê¸° (Shape: {grads[i*2].shape}):\n{np.array2string(grads[i*2].numpy(), threshold=np.inf, max_line_width=np.inf)}\n")
                        f.write(f"  - í¸í–¥ ê¸°ìš¸ê¸° (Shape: {grads[i*2+1].shape}):\n{np.array2string(grads[i*2+1].numpy(), threshold=np.inf, max_line_width=np.inf)}\n")

                    # 3-4. íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ê¸°ë¡
                    optimizer.apply_gradients(zip(grads, trainable_vars))
                    
                    f.write("\n--[ 3-4. íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ í›„ ]--\n")
                    for i, layer in enumerate(all_layers):
                        layer_name = 'ì€ë‹‰ì¸µ' if i == 0 else 'ì¶œë ¥ì¸µ'
                        f.write(f"\n[{layer_name}ì˜ ìƒˆ íŒŒë¼ë¯¸í„°]\n")
                        f.write(f"  - ìƒˆ ê°€ì¤‘ì¹˜:\n{np.array2string(layer.kernel.numpy(), threshold=np.inf, max_line_width=np.inf)}\n")
                        f.write(f"  - ìƒˆ í¸í–¥:\n{np.array2string(layer.bias.numpy(), threshold=np.inf, max_line_width=np.inf)}\n")
                    
                f.write(f"\n{'='*25} STEP {step+1} ì¢…ë£Œ {'='*25}\n")
            
            f.write(f"\n\n--- ìµœì¢… ì—í¬í¬ ì •í™•ë„: {accuracy_metric.result().numpy()} ---\n")

    print(f"[{model_name}] ì‹¤í—˜ ì™„ë£Œ. ë¡œê·¸ê°€ ë‹¤ìŒ íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:\n{log_file_path}\n")


# 3. ë©”ì¸ ì‹¤í–‰ ë¡œì§
if __name__ == "__main__":
    # ì‹¤í—˜ A: ê°€ì„¤(ëŒ€ì¹­) ëª¨ë¸ ì‹¤í–‰
    run_experiment(model_type='symmetric', log_file_name='hypothesis_model_log.txt')
    
    # ì‹¤í—˜ B: í‘œì¤€(ë¬´ì‘ìœ„) ëª¨ë¸ ì‹¤í–‰
    run_experiment(model_type='standard', log_file_name='standard_model_log.txt')

    print("="*60)
    print("ëª¨ë“  ì‹¤í—˜ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("="*60)