신경망의 '대칭성 문제'와 가중치 초기화의 중요성 검증
1. 실험 개요
본 실험은 신경망 학습의 가장 근본적인 원리 중 하나인 가중치 초기화의 중요성을 탐구하는 것을 목표로 합니다. "왜 신경망의 가중치를 복잡한 무작위 값으로 설정해야 하는가?"라는 질문에서 출발하여, 모든 가중치를 '1'이라는 동일하고 단순한 값으로 설정했을 때 발생하는 **'대칭성 문제(Symmetry Problem)'**를 실험적으로 증명하고, 그 과정을 심층적으로 분석하고자 합니다.

이를 위해, 잘못된 초기화 방식을 사용한 모델과 표준 초기화 방식을 사용한 모델을 직접 구현하고, 두 모델의 학습 성능과 내부 동작 메커니즘을 비교하여, '무작위성'이 신경망 학습에 필수적인 이유를 명확히 규명합니다.

2. 핵심 가설
만약 신경망의 모든 가중치를 동일한 값(e.g., 1)으로 설정하면, 모든 뉴런이 학습 과정에서 동일하게 변화하여 개별적인 특징을 학습할 수 없게 되어, 결국 모델 전체의 학습이 실패할 것이다.

3. 실험 설계
본 실험은 초기화 방식 외 모든 조건을 동일하게 통제한 두 모델을 비교하는 A/B 테스트로 설계됩니다.

가. 비교 대상
모델 A (가설 모델): 모든 가중치를 1, 모든 편향을 0으로 초기화한 실험군. '대칭성 문제'가 발생할 것으로 예상되는 모델입니다.

모델 B (표준 모델): 일반적인 무작위 초기화(Keras 기본값)를 사용한 대조군. 정상적인 학습이 진행될 것으로 예상되는 모델입니다.

나. 데이터 및 과제
데이터: 표준 이미지 분류 데이터셋인 MNIST를 사용합니다.

과제: 문제의 핵심을 명확히 관찰하기 위해, 다중 분류 문제를 이진 분류 문제로 단순화합니다. 즉, 주어진 이미지가 **'숫자 3인지 아닌지'**를 판별하는 과제를 수행합니다.

'3' 레이블 → 1

'3'이 아닌 숫자 레이블 → 0

다. 측정 지표 및 기록 방식
단순한 성능 비교를 넘어, 학습 실패의 '원인'을 규명하기 위해 모델의 내부 상태를 모두 기록하는 '블랙박스 로깅' 방식을 채택합니다.

성능 지표: 각 모델의 에포크(Epoch)별 **훈련 정확도(Accuracy)**를 측정하여 학습 곡선을 비교합니다.

내부 동작 지표: 학습의 각 스텝(Step)마다 아래의 모든 값을 텍스트 파일에 상세히 기록합니다.

순전파 기록: 각 층의 가중치, 편향, 활성화 값(노드 값)

역전파 기록: 각 가중치와 편향에 대한 기울기(Gradient) 값

업데이트 기록: 기울기가 적용된 후의 새로운 가중치, 편향 값

4. 예상 결과 및 의의
성능 예측: **모델 B(표준 모델)**는 95% 이상의 높은 정확도를 달성하며 성공적으로 학습하는 반면, **모델 A(대칭 모델)**는 학습이 진전되지 않고 정확도가 데이터의 다수 클래스 비율인 약 90% 근처에서 정체될 것으로 예상됩니다.

내부 로그 분석 예측: 모델 A의 로그에서는 모든 뉴런의 기울기 값과 업데이트되는 가중치 값이 항상 동일하게 기록될 것입니다. 반면 모델 B의 로그에서는 모든 값이 제각각 다르게 기록되어, 각 뉴런이 개별적으로 학습하고 있음을 보여줄 것입니다.

본 실험을 통해 우리는 '가중치 무작위 초기화'가 단순히 기술적인 관례가 아닌, 신경망 내 '다양성'을 확보하여 각 뉴런의 전문화를 가능하게 하는 핵심적인 철학임을 입증할 수 있습니다. 이는 딥러닝 모델의 학습 원리를 가장 근본적인 수준에서 이해하는 데 중요한 기여를 할 것입니다.
