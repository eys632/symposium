{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da368cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch (Sigmoid) 예측 결과:\n",
      " [[0.01603173]\n",
      " [0.9808172 ]\n",
      " [0.9807922 ]\n",
      " [0.03328492]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "X = torch.tensor([[0,0], [0,1], [1,0], [1,1]], dtype=torch.float32)\n",
    "y = torch.tensor([[0],   [1],   [1],   [0]],   dtype=torch.float32)\n",
    "\n",
    "class XORModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(2, 2)\n",
    "        \n",
    "        # [수정됨] 활성화 함수: ReLU -> Sigmoid\n",
    "        # 아까 수식에서 본 S자 곡선을 적용합니다.\n",
    "        self.activation = nn.Sigmoid() \n",
    "        \n",
    "        self.layer2 = nn.Linear(2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation(x) # 여기서 부드럽게 휜 공간이 만들어집니다.\n",
    "        x = self.layer2(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "model = XORModel()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(2000):\n",
    "# [Step 1] 순전파 (Forward)\n",
    "    # 모델에 X를 넣고 예측값(prediction)을 받아옵니다.\n",
    "    prediction = model(X)\n",
    "    \n",
    "    # [Step 2] 오차 계산 (Loss Calculation)\n",
    "    # 예측값과 정답(y)을 비교해 Loss(J)를 구합니다.\n",
    "    loss = criterion(prediction, y)\n",
    "    \n",
    "    # [Step 3] 기울기 초기화 (Zero Grad) - 중요!\n",
    "    # PyTorch는 기울기를 계속 누적하는 성질이 있습니다.\n",
    "    # 이전 에폭의 찌꺼기 기울기를 0으로 깨끗이 지워줍니다.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # [Step 4] 역전파 (Backward) - 체인 룰 발동!\n",
    "    # Loss에서부터 거꾸로 미분하며 각 파라미터(W)별 기울기를 계산합니다.\n",
    "    # 계산된 기울기는 각 파라미터 내부에 저장됩니다.\n",
    "    loss.backward()\n",
    "    \n",
    "    # [Step 5] 업데이트 (Step)\n",
    "    # W_new = W_old - (lr * Gradient)\n",
    "    # 계산된 기울기 방향으로 파라미터를 한 걸음 수정합니다.\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"PyTorch (Sigmoid) 예측 결과:\\n\", model(X).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e59d0482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== [학습 시작 전 가중치(Weights)] ====================\n",
      "Layer 1 Weights:\n",
      "tensor([[ 0.5406,  0.5869],\n",
      "        [-0.1657,  0.6496]])\n",
      "Layer 2 Weights:\n",
      "tensor([[-0.3443,  0.4153]])\n",
      "\n",
      "==================== [1. 순전파 (Forward Pass)] ====================\n",
      "입력 데이터 X가 들어갑니다.\n",
      "\n",
      "▶ Step 1 [Linear 1]: 입력 공간을 늘리고 회전시킴 (z1)\n",
      "[[-0.15492964  0.14268756]\n",
      " [ 0.4319746   0.7922438 ]\n",
      " [ 0.38568074 -0.02296811]\n",
      " [ 0.97258496  0.6265881 ]]\n",
      "\n",
      "▶ Step 2 [Activation]: 공간을 부드럽게 휘게 만듦 (h1)\n",
      "[[0.46134487 0.5356115 ]\n",
      " [0.60634506 0.68831295]\n",
      " [0.59524244 0.49425822]\n",
      " [0.7256344  0.65171546]]\n",
      "\n",
      "▶ Step 3 [Linear 2]: 휘어진 공간에서 영역을 합침 (z2)\n",
      "[[0.68694735]\n",
      " [0.7004423 ]\n",
      " [0.6236791 ]\n",
      " [0.6441781 ]]\n",
      "\n",
      "▶ Step 4 [Output]: 최종 확률값 예측 (y_pred)\n",
      "[[0.6652875 ]\n",
      " [0.66828585]\n",
      " [0.65105486]\n",
      " [0.6556973 ]]\n",
      "\n",
      "==================== [2. 오차 계산 (Loss)] ====================\n",
      "Loss 값: 0.7482\n",
      "(이 오차를 줄이기 위해 역전파를 시작합니다!)\n",
      "\n",
      "==================== [3. 역전파 (Backward Pass)] ====================\n",
      "체인 룰(Chain Rule)을 타고 오차 신호(Gradient)가 거꾸로 흐릅니다.\n",
      "\n",
      "◀ Gradient @ Output (dL/dy_pred): 결과가 얼마나 틀렸는지\n",
      "[[ 0.16632186]\n",
      " [-0.08292854]\n",
      " [-0.08723629]\n",
      " [ 0.16392432]]\n",
      "\n",
      "◀ Gradient @ Layer 2 Weights (dL/dW2): 출력층 가중치를 얼마나 수정해야 할지\n",
      "[[0.0934708  0.09571788]]\n",
      "\n",
      "◀ Gradient @ Hidden Layer (dL/dh1): 은닉층이 얼마나 잘못했는지\n",
      "[[-0.05725771  0.06906874]\n",
      " [ 0.02854885 -0.03443787]\n",
      " [ 0.03003183 -0.03622675]\n",
      " [-0.05643234  0.06807311]]\n",
      "\n",
      "◀ Gradient @ Layer 1 Weights (dL/dW1): 은닉층 가중치를 얼마나 수정해야 할지\n",
      "[[-0.00399953 -0.00442072]\n",
      " [ 0.00639591  0.00806316]]\n",
      "\n",
      "==================== [4. 파라미터 업데이트 (Update)] ====================\n",
      "학습률(Learning Rate)을 곱해서 가중치를 수정했습니다.\n",
      "\n",
      "Updated Layer 1 Weights:\n",
      "tensor([[ 0.5410,  0.5873],\n",
      "        [-0.1663,  0.6487]])\n",
      "Updated Layer 2 Weights:\n",
      "tensor([[-0.3536,  0.4057]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. 데이터 준비 (XOR)\n",
    "X = torch.tensor([[0,0], [0,1], [1,0], [1,1]], dtype=torch.float32)\n",
    "y = torch.tensor([[0],   [1],   [1],   [0]],   dtype=torch.float32)\n",
    "\n",
    "# 2. 모델 정의 (구조만 정의)\n",
    "class XORModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 가중치 초기값을 고정 (발표 때 항상 같은 결과가 나오도록)\n",
    "        torch.manual_seed(42) \n",
    "        \n",
    "        self.layer1 = nn.Linear(2, 2)  # 은닉층 (입력2 -> 은닉2)\n",
    "        self.activation = nn.Sigmoid() # 활성화 (Sigmoid)\n",
    "        self.layer2 = nn.Linear(2, 1)  # 출력층 (은닉2 -> 출력1)\n",
    "        self.sigmoid = nn.Sigmoid()    # 최종 확률\n",
    "\n",
    "# 모델 생성\n",
    "model = XORModel()\n",
    "criterion = nn.BCELoss()\n",
    "lr = 0.1 # 학습률\n",
    "\n",
    "print(f\"{'='*20} [학습 시작 전 가중치(Weights)] {'='*20}\")\n",
    "print(f\"Layer 1 Weights:\\n{model.layer1.weight.data}\")\n",
    "print(f\"Layer 2 Weights:\\n{model.layer2.weight.data}\\n\")\n",
    "\n",
    "# =================================================================\n",
    "# ★ 여기가 핵심! (모델 내부를 한 단계씩 뜯어서 실행)\n",
    "# =================================================================\n",
    "\n",
    "print(f\"{'='*20} [1. 순전파 (Forward Pass)] {'='*20}\")\n",
    "print(\"입력 데이터 X가 들어갑니다.\\n\")\n",
    "\n",
    "# Step 1: 첫 번째 선형 변환 (z1 = W1*x + b1)\n",
    "z1 = model.layer1(X)\n",
    "# [중요] 중간 변수의 기울기를 기억하도록 설정 (원래는 버려짐)\n",
    "z1.retain_grad() \n",
    "print(f\"▶ Step 1 [Linear 1]: 입력 공간을 늘리고 회전시킴 (z1)\\n{z1.detach().numpy()}\\n\")\n",
    "\n",
    "# Step 2: 활성화 함수 (h1 = Sigmoid(z1))\n",
    "h1 = model.activation(z1)\n",
    "h1.retain_grad() # 기울기 기억\n",
    "print(f\"▶ Step 2 [Activation]: 공간을 부드럽게 휘게 만듦 (h1)\\n{h1.detach().numpy()}\\n\")\n",
    "\n",
    "# Step 3: 두 번째 선형 변환 (z2 = W2*h1 + b2)\n",
    "z2 = model.layer2(h1)\n",
    "z2.retain_grad() # 기울기 기억\n",
    "print(f\"▶ Step 3 [Linear 2]: 휘어진 공간에서 영역을 합침 (z2)\\n{z2.detach().numpy()}\\n\")\n",
    "\n",
    "# Step 4: 최종 출력 (y_pred = Sigmoid(z2))\n",
    "y_pred = model.sigmoid(z2)\n",
    "print(f\"▶ Step 4 [Output]: 최종 확률값 예측 (y_pred)\\n{y_pred.detach().numpy()}\\n\")\n",
    "\n",
    "\n",
    "print(f\"{'='*20} [2. 오차 계산 (Loss)] {'='*20}\")\n",
    "loss = criterion(y_pred, y)\n",
    "print(f\"Loss 값: {loss.item():.4f}\")\n",
    "print(\"(이 오차를 줄이기 위해 역전파를 시작합니다!)\\n\")\n",
    "\n",
    "\n",
    "print(f\"{'='*20} [3. 역전파 (Backward Pass)] {'='*20}\")\n",
    "# 역전파 실행! (모든 기울기가 계산됨)\n",
    "loss.backward()\n",
    "\n",
    "print(\"체인 룰(Chain Rule)을 타고 오차 신호(Gradient)가 거꾸로 흐릅니다.\\n\")\n",
    "\n",
    "# Gradient 확인 (뒤에서부터)\n",
    "print(f\"◀ Gradient @ Output (dL/dy_pred): 결과가 얼마나 틀렸는지\\n{z2.grad.numpy()}\\n\") # z2 입장에서의 변화량\n",
    "\n",
    "print(f\"◀ Gradient @ Layer 2 Weights (dL/dW2): 출력층 가중치를 얼마나 수정해야 할지\")\n",
    "print(f\"{model.layer2.weight.grad.numpy()}\\n\")\n",
    "\n",
    "print(f\"◀ Gradient @ Hidden Layer (dL/dh1): 은닉층이 얼마나 잘못했는지\")\n",
    "print(f\"{h1.grad.numpy()}\\n\") # 여기가 중요! 역전파가 은닉층까지 도달함\n",
    "\n",
    "print(f\"◀ Gradient @ Layer 1 Weights (dL/dW1): 은닉층 가중치를 얼마나 수정해야 할지\")\n",
    "print(f\"{model.layer1.weight.grad.numpy()}\\n\")\n",
    "\n",
    "\n",
    "print(f\"{'='*20} [4. 파라미터 업데이트 (Update)] {'='*20}\")\n",
    "# 수동으로 업데이트 (W = W - lr * grad)\n",
    "with torch.no_grad(): # 업데이트는 기록하지 않음\n",
    "    model.layer1.weight -= lr * model.layer1.weight.grad\n",
    "    model.layer2.weight -= lr * model.layer2.weight.grad\n",
    "\n",
    "print(\"학습률(Learning Rate)을 곱해서 가중치를 수정했습니다.\\n\")\n",
    "print(f\"Updated Layer 1 Weights:\\n{model.layer1.weight.data}\")\n",
    "print(f\"Updated Layer 2 Weights:\\n{model.layer2.weight.data}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "use_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
